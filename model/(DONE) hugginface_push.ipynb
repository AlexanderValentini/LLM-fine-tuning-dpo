{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You have loaded an AWQ model on CPU and have a CUDA device available, make sure to set your model on a GPU device in order to run your model.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "#Code origin\n",
    "#Author: Alexander Valentini\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "#Using quantization, but this should probably not be in the final version:\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "#FOR QUANTIZATION:\n",
    "\n",
    "#bnb_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,\n",
    "#    llm_int8_threshold=6.0,\n",
    "#    llm_int8_has_fp16_weight=False,\n",
    "#    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "#    bnb_4bit_quant_type=\"nf4\",\n",
    "#)\n",
    "\n",
    "#model_name = 'AlexVal/dpo_model'\n",
    "\n",
    "safetensors_path = 'models/mcqa_model_full_3B_sft_mcqa'\n",
    "\n",
    "#FOR LOADING IN AND MERGING LORA ADAPTER:\n",
    "\n",
    "#os.path.abspath(os.getcwd())\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     safetensors_path,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     attn_implementation=\"sdpa\",\n",
    "\n",
    "#).to(device)\n",
    "\n",
    "#merged_model = model.merge_and_unload()\n",
    "#merged_model.resize_token_embeddings(len(tokenizer))\n",
    "#merged_model.save_pretrained('dpo_model_1.6B/merged_model',safe_serialization=True)\n",
    "\n",
    "\n",
    "#LOADING IN TOKENIZER AND MODEL:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"new_tokenizer\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(safetensors_path, torch_dtype=torch.float16, use_safetensors = True).to(device)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.path.exists('D:\\shared\\DTU\\10. Semester\\Modern Natural Language Processing\\Project\\Milestone 3\\github_project\\project-m3-2024-dreamteam\\model\\mcqa_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StableLmForCausalLM(\n",
       "  (model): StableLmModel(\n",
       "    (embed_tokens): Embedding(50278, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x StableLmDecoderLayer(\n",
       "        (self_attn): StableLmSdpaAttention(\n",
       "          (q_proj): WQLinear_GEMM(in_features=2560, out_features=2560, bias=False, w_bit=4, group_size=128)\n",
       "          (k_proj): WQLinear_GEMM(in_features=2560, out_features=2560, bias=False, w_bit=4, group_size=128)\n",
       "          (v_proj): WQLinear_GEMM(in_features=2560, out_features=2560, bias=False, w_bit=4, group_size=128)\n",
       "          (o_proj): WQLinear_GEMM(in_features=2560, out_features=2560, bias=False, w_bit=4, group_size=128)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (rotary_emb): StableLmRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): StableLmMLP(\n",
       "          (gate_proj): WQLinear_GEMM(in_features=2560, out_features=6912, bias=False, w_bit=4, group_size=128)\n",
       "          (up_proj): WQLinear_GEMM(in_features=2560, out_features=6912, bias=False, w_bit=4, group_size=128)\n",
       "          (down_proj): WQLinear_GEMM(in_features=6912, out_features=2560, bias=False, w_bit=4, group_size=128)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=50278, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.path.isdir('sft_stablelm_zephyr_3b_debug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\MNLP\\lib\\site-packages\\transformers\\models\\stablelm\\modeling_stablelm.py:485: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "List 3 synonyms for the word \"tiny\"<|endoftext|>\n",
      "<|assistant|>\n",
      "1.\n",
      "2. Small\n",
      "3. Petite<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "#model = merged_model\n",
    "prompt = [{'role': 'user', 'content': 'List 3 synonyms for the word \"tiny\"'}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "tokens = model.generate(\n",
    "    inputs.to(model.device),\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.8,\n",
    "    do_sample=True,\n",
    "     pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df87d5331e914ce0b621a034f2aa017c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438778a2bf6e4910a1df997a59d8a10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/AlexVal/mcqa_model_full_only_mcqa-awq_newest/commit/15faa6118b6a38da5120978b925aca15e2d5fcec', commit_message='Commiting model', commit_description='', oid='15faa6118b6a38da5120978b925aca15e2d5fcec', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "model.push_to_hub(\n",
    " repo_id=\"mcqa_model_full_only_mcqa-awq_newest\",\n",
    " commit_message=\"Commiting model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600dc9717ffd4f6e9c24c850fb867bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\MNLP\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alex\\.cache\\huggingface\\hub\\models--AlexVal--mcqa_model_full_only_mcqa-awq_newest. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/AlexVal/mcqa_model_full_only_mcqa-awq_newest/commit/258fba6a3e0eb9d000c8c7d339ac45460b6e0361', commit_message='new_tokenizer', commit_description='', oid='258fba6a3e0eb9d000c8c7d339ac45460b6e0361', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\n",
    " repo_id=\"mcqa_model_full_only_mcqa-awq_newest\",\n",
    " commit_message=\"new_tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
