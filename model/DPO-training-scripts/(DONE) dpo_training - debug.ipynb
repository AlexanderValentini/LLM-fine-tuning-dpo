{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9370c29b88c749eb80e661fd019f1a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#Code origin\n",
    "#Author: Alexander Valentini\n",
    "#The preference data is transformed into the huggingface DPO format - and all pairs where the prompt is longer than 900 tokens\n",
    "#or one of the answers are longer than 1024 tokens are removed. \n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, DataCollatorWithPadding#, BitsAndBytesConfig \n",
    "from datasets import load_from_disk, load_dataset, Dataset\n",
    "import numpy as np\n",
    "#Using quantization, but this should probably not be in the final version:\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from trl import DPOTrainer\n",
    "\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "#bnb_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,\n",
    "#    llm_int8_threshold=6.0,\n",
    "#    llm_int8_has_fp16_weight=False,\n",
    "#    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "#    bnb_4bit_quant_type=\"nf4\",\n",
    "#)\n",
    "\n",
    "model_name = 'stabilityai/stablelm-zephyr-3b'\n",
    "safetensors_path = 'AlexVal/sft-model'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "model = AutoModelForCausalLM.from_pretrained(safetensors_path, attn_implementation=\"sdpa\",\n",
    "    torch_dtype=torch.bfloat16, use_safetensors = True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation=\"sdpa\",\n",
    "#    torch_dtype=torch.bfloat16, use_safetensors = True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StableLmForCausalLM(\n",
       "  (model): StableLmModel(\n",
       "    (embed_tokens): Embedding(50278, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x StableLmDecoderLayer(\n",
       "        (self_attn): StableLmSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (o_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (rotary_emb): StableLmRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): StableLmMLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=50278, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "d:\\ProgramData\\Anaconda3\\envs\\MNLP\\lib\\site-packages\\transformers\\models\\stablelm\\modeling_stablelm.py:485: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "List 3 synonyms for the word \"tiny\"<|endoftext|>\n",
      "<|assistant|>\n",
      "1.\n",
      "2.\n",
      "3.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = [{'role': 'user', 'content': 'List 3 synonyms for the word \"tiny\"'}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "tokens = model.generate(\n",
    "    inputs.to(model.device),\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.8,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in training and evaluation data:\n",
    "import os\n",
    "\n",
    "project_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "#data_dir = os.path.join(project_dir, 'datasets\\dpo_preference_data_self_made')\n",
    "#preference_data = load_from_disk(data_dir)\n",
    "#train_dataset = preference_data['train']\n",
    "#eval_dataset = preference_data['eval']\n",
    "\n",
    "train_data_path = os.path.join(project_dir, 'datasets/dpo_preference_data_self_made/train_preference_data.jsonl')\n",
    "vali_data_path = os.path.join(project_dir, 'datasets/dpo_preference_data_self_made/eval_preference_data.jsonl')\n",
    "\n",
    "dataset=load_dataset('json', data_files={\"train\":train_data_path, \"validation\":vali_data_path})\n",
    "\n",
    "train_dataset = load_dataset('json',data_files=train_data_path)\n",
    "eval_dataset = load_dataset('json',data_files=vali_data_path)\n",
    "\n",
    "#train_dataset\n",
    "#print(f\"Number of examples in the training set: {len(train_dataset)}\")\n",
    "#print(f\"Number of examples in the evaluation set: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '<|user|>\\nQuestion: Let $X$ denote the random variable associated to the plaintexts and $Y$ the random variable associated to the corresponding ciphertexts. If a cryptosystem achieves perfect secrecy, then we have that \\\\dots?\\n\\nOptions:\\nA. $\\\\Pr [X=x|Y=y] = \\\\Pr[Y=y]$.\\nB. $\\\\Pr [X=x|Y=y] = \\\\Pr[X=x]$.\\nC. $\\\\Pr [X=x|Y=y] = \\\\Pr[X=x,Y=y]$.\\nD. $\\\\Pr [X=x] = \\\\Pr[Y=y]$.<|endoftext|',\n",
       " 'chosen': '<|assistant|>\\nThe correct statement is: \\\\\"$\\\\\\\\\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\\\\\\\\\Pr[X=x]$.\\\\\"\\\\n\\\\nPerfect secrecy in a cryptosystem means that observing the ciphertext does not give any information about the plaintext. Mathematically, this is formalized as $\\\\\\\\\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\\\\\\\\\Pr[X=x]$, meaning that the probability of a certain plaintext being the true message given the ciphertext is the same as the probability of that plaintext being the true message without knowing the ciphertext.\\\\n\\\\nThe other statements are not necessarily true in the context of perfect secrecy. \\\\\"$\\\\\\\\\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\\\\\\\\\Pr[Y=y]$\\\\\" is not correct because if the probability of the plaintext given the ciphertext was equal to the probability of the ciphertext, it would mean that the ciphertext reveals information about the plaintext. \\\\\"$\\\\\\\\\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\\\\\\\\\Pr[X=x,Y=y]$\\\\\" is also not correct because it implies that the probability of the plaintext given the ciphertext is the same as the joint probability of the plaintext and ciphertext, which may not be true. Finally, \\\\\"$\\\\\\\\\\\\\\\\Pr [X=x] = \\\\\\\\\\\\\\\\Pr[Y=y]$\\\\\" is not necessarily true as well because perfect secrecy does not require the marginal probabilities of the plaintexts and ciphertexts to be equal.<|endoftext|',\n",
       " 'rejected': '<|assistant|>\\nI apologize for the mistake in my previous response. The correct statement should be: \\n\\nCorrect statement: \"$\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\Pr[X=x]$.\"\\n\\nJustification: In a cryptosystem that achieves perfect secrecy, the knowledge of the ciphertext does not reveal any information about the plaintext. Therefore, the probability of a specific plaintext given a specific ciphertext should be the same as the probability of that specific plaintext occurring. This is reflected in the correct statement. Thank you for pointing out the error.<|endoftext|'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up Lora:\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\MNLP\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Training arguments for DPO:\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/dpo_model_test\",               # directory to save and repository id\n",
    "    num_train_epochs=1,                     # number of training epochs\n",
    "    per_device_train_batch_size=12,         # batch size per device during training\n",
    "    per_device_eval_batch_size=4,           # batch size for evaluation\n",
    "    gradient_accumulation_steps=1,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    learning_rate=5e-5,                     # 10x higher LR than QLoRA paper\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.1,                       # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"cosine\",             # use cosine learning rate scheduler\n",
    "    logging_steps=25,                       # log every 25 steps\n",
    "    save_steps=400,                         # when to save checkpoint\n",
    "    save_total_limit=4,                     # limit the total amount of checkpoints\n",
    "    evaluation_strategy=\"steps\",            # evaluate every 1000 steps\n",
    "    eval_steps=700,                         # when to evaluate\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    push_to_hub=False,                      # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")\n",
    " \n",
    "dpo_args = {\n",
    "    \"beta\": 0.1,                            # The beta factor in DPO loss. Higher beta means less divergence\n",
    "    \"loss_type\": \"sigmoid\"                  # The loss type for DPO.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593\n",
      "704\n",
      "755\n"
     ]
    }
   ],
   "source": [
    "#Finding 99th percentile of data length for padding:\n",
    "prompt_length = int(np.percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset['train'][\"prompt\"]], 99))\n",
    "rejected_length = int(np.percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset['train'][\"rejected\"]], 99))\n",
    "chosen_length = int(np.percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset['train'][\"chosen\"]], 99))\n",
    "\n",
    "print(prompt_length)\n",
    "print(rejected_length)\n",
    "print(chosen_length)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This should be increased in later versions:\n",
    "prompt_max_length = 900\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\MNLP\\lib\\site-packages\\trl\\trainer\\dpo_trainer.py:332: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f9081678664975baa8f9223ec9b4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None, # set to none since we use peft\n",
    "    peft_config=peft_config,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_seq_length,\n",
    "    max_prompt_length=prompt_max_length,\n",
    "    beta=dpo_args[\"beta\"],\n",
    "    loss_type=dpo_args[\"loss_type\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bf06defd994d70848165e5867c0493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\MNLP\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\Anaconda3\\envs\\MNLP\\lib\\site-packages\\transformers\\models\\stablelm\\modeling_stablelm.py:485: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = Path(args.output_dir)\n",
    "if checkpoint_dir.exists():\n",
    "    checkpoints = list(checkpoint_dir.glob(\"checkpoint-*\"))\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda path: int(path.name.split(\"-\")[-1]))\n",
    "        print(f\"Loading from checkpoint {latest_checkpoint}\")\n",
    "    else:\n",
    "        latest_checkpoint = None\n",
    "else:\n",
    "    latest_checkpoint = None\n",
    "\n",
    "# Continue training from the latest checkpoint if available\n",
    "if latest_checkpoint:\n",
    "    trainer.train(resume_from_checkpoint=str(latest_checkpoint))\n",
    "else:\n",
    "    trainer.train()\n",
    "\n",
    "# save model at the end of training\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.save_pretrained(args.output_dir, safe_serialization=True, max_shard_size=\"2GB\")\n",
    "tokenizer.save_pretrained(args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
