{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#Code origin\n",
    "#Author: Alexander Valentini\n",
    "\n",
    "from preference_data_parser import parse_preference_data\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "from utils import write_jsonl\n",
    "\n",
    "content = parse_preference_data()\n",
    "model_name='stabilityai/stablelm-zephyr-3b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26738"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#content is a list of dictionaries, each dictionary has the following keys:\n",
    "# 'preference' : The preference answer pairs for the question\n",
    "# 'question_complete' : The question text\n",
    "# 'question_id' : The question ID\n",
    "# 'course id' : The course ID\n",
    "#There are 1522 different questions (in the list) and  18 preference pair answers for each question (accessed by ['preference']). \n",
    "total_length = 0\n",
    "for key in content:\n",
    "    total_length += len(key['preference'])\n",
    "total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0]['preference'][0]['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_triplets(content, tokenizer):\n",
    "    triplets = []\n",
    "\n",
    "    #Add assistant message.\n",
    "    for item in content:\n",
    "        question = item['question_complete']\n",
    "        for pref in item['preference']:\n",
    "            chosen = pref['B'] if pref['overall'] == 'B' else pref['A']\n",
    "            rejected = pref['A'] if pref['overall'] == 'B' else pref['B']\n",
    "            prompt_messages = [{\"role\": \"user\", \"content\": question}]\n",
    "            chosen_messages = [{\"role\": \"assistant\", \"content\": chosen}]\n",
    "            rejected_messages = [{\"role\": \"assistant\", \"content\": rejected}]\n",
    "            \n",
    "            triplet = {\n",
    "                \"prompt\": tokenizer.apply_chat_template(prompt_messages, tokenize=False)[:-2],#+\"<|assistant|>\",\n",
    "                \"chosen\": tokenizer.apply_chat_template(chosen_messages, tokenize=False)[:-2],#[14:][:-2],\n",
    "                \"rejected\": tokenizer.apply_chat_template(rejected_messages, tokenize=False)[:-2]#[14:][:-2]\n",
    "            }\n",
    "            triplets.append(triplet)\n",
    "    \n",
    "    return triplets\n",
    "\n",
    "# Convert the data\n",
    "converted_data = convert_to_triplets(content, tokenizer)\n",
    "\n",
    "# Split the data into train (80%), temp (20%)\n",
    "train_data, temp_data = train_test_split(converted_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temp data into eval (50% of temp) and test (50% of temp)\n",
    "eval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(eval_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# Combine the datasets into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"eval\": eval_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# Save to a single JSON file\n",
    "#output_path = \"datasets/dpo_preference_data_self_made\"\n",
    "#dataset_dict.save_to_disk(output_path)\n",
    "\n",
    "# Verify the saved data\n",
    "#print(f\"Saved dataset splits to {output_path}\")\n",
    "\n",
    "#Number of entries with prompt length greater than 1024\n",
    "#print(sum([len(tokenizer(x)[\"input_ids\"]) > 1024 for x in train_dataset[\"prompt\"]]))\n",
    "#print(sum([len(tokenizer(x)[\"input_ids\"]) > 1024 for x in train_dataset[\"chosen\"]]))\n",
    "#print(sum([len(tokenizer(x)[\"input_ids\"]) > 1024 for x in train_dataset[\"rejected\"]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_jsonl(train_data, 'train_preference_data_new.jsonl')\n",
    "#write_jsonl(eval_data, 'eval_preference_data_new.jsonl')\n",
    "#write_jsonl(test_data, 'test_preference_data_new.jsonl')\n",
    "\n",
    "#test_dataset.to_json('test_preference_data_new.jsonl', orient=\"records\")\n",
    "#train_dataset.to_json('train_preference_data_new.jsonl', orient=\"records\")\n",
    "#eval_dataset.to_json('eval_preference_data_new.jsonl', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 21390\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length = 2500\n",
    "train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_length_max = int(np.max([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"prompt\"]]))\n",
    "chosen_length_max = int(np.max([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"chosen\"]]))\n",
    "rejected_length_max = int(np.max([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"rejected\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081\n",
      "1983\n",
      "2085\n"
     ]
    }
   ],
   "source": [
    "print(prompt_length_max)\n",
    "print(chosen_length_max)\n",
    "print(rejected_length_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2085 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "prompt_length_99 = int(np.percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"prompt\"]], 99))\n",
    "chosen_length_99 = int(np.percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"chosen\"]], 99))\n",
    "rejected_length_99 = int(np.percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"rejected\"]], 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n",
      "806\n",
      "732\n",
      "48\n",
      "43\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(prompt_length_99)\n",
    "print(chosen_length_99)\n",
    "print(rejected_length_99)\n",
    "print(sum([len(tokenizer(x)[\"input_ids\"]) > 1024 for x in train_dataset[\"prompt\"]]))\n",
    "print(sum([len(tokenizer(x)[\"input_ids\"]) > 1024 for x in train_dataset[\"chosen\"]]))\n",
    "print(sum([len(tokenizer(x)[\"input_ids\"]) > 1024 for x in train_dataset[\"rejected\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287\n",
      "1202\n",
      "1287\n"
     ]
    }
   ],
   "source": [
    "#prompt_length = int(percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"prompt\"]], 95))\n",
    "max_seq_length_chosen = int(np.percentile([len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) for x in train_dataset], 99))\n",
    "max_seq_length_rejected = int(np.percentile([len(tokenizer(x[\"prompt\"] + x[\"rejected\"])[\"input_ids\"]) for x in train_dataset], 99))\n",
    "max_seq_length = max(max_seq_length_chosen, max_seq_length_rejected)\n",
    "print(max_seq_length_chosen)\n",
    "print(max_seq_length_rejected)\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d678d2540c147b2a2596998a8f9041b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/21390 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474903b7b62f470e883db13fe9d488f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2674 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Please remove the entries with prompt length+chosen length or prompt length+rejected length greater than max_seq_length\n",
    "train_dataset_filtered = train_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length and len(tokenizer(x[\"prompt\"] + x[\"rejected\"])[\"input_ids\"]) <= max_seq_length)\n",
    "eval_dataset_filtered = eval_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length and len(tokenizer(x[\"prompt\"] + x[\"rejected\"])[\"input_ids\"]) <= max_seq_length)\n",
    "\n",
    "\n",
    "#train_dataset_filtered = train_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"])[\"input_ids\"]) <= 1024 and len(tokenizer(x[\"chosen\"])[\"input_ids\"]) <= 1024 and len(tokenizer(x[\"rejected\"])[\"input_ids\"]) <= 1024)\n",
    "#eval_dataset_filtered = eval_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"])[\"input_ids\"]) <= 1024 and len(tokenizer(x[\"chosen\"])[\"input_ids\"]) <= 1024 and len(tokenizer(x[\"rejected\"])[\"input_ids\"]) <= 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1081"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find max prompt length\n",
    "prompt_length_max = int(np.max([len(tokenizer(x[\"prompt\"])[\"input_ids\"]) for x in train_dataset_filtered]))\n",
    "prompt_length_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the datasets into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset_filtered,\n",
    "    \"eval\": eval_dataset_filtered,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# Save to a single JSON file\n",
    "output_path = \"datasets/dpo_preference_data_self_made/preference_data_99_percentile_filtered\"\n",
    "dataset_dict.save_to_disk(output_path)\n",
    "\n",
    "# Verify the saved data\n",
    "print(f\"Saved dataset splits to {output_path}\")\n",
    "\n",
    "#Find max sequence length\n",
    "max_seq_length_chosen = int(np.max([len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) for x in train_dataset_filtered]))\n",
    "max_seq_length_rejected = int(np.max([len(tokenizer(x[\"prompt\"] + x[\"rejected\"])[\"input_ids\"]) for x in train_dataset_filtered]))\n",
    "print(max_seq_length_chosen)\n",
    "print(max_seq_length_rejected)\n",
    "\n",
    "#Number of entries with prompt length greater than 1024\n",
    "#print(sum([len(tokenizer(x)[\"input_ids\"]) > 1024 for x in train_dataset_filtered[\"prompt\"]]))\n",
    "#print(sum([len(tokenizer(x)[\"input_ids\"]) > 1024 for x in train_dataset_filtered[\"chosen\"]]))\n",
    "#print(sum([len(tokenizer(x)[\"input_ids\"]) > 1024 for x in train_dataset_filtered[\"rejected\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please write code to find the length of all the prompt, chosen, and rejected messages in the train_dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in the training set: 21282\n",
      "Number of examples in the evaluation set: 2663\n",
      "Number of examples in the test set: 2674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': '<|user|>\\nQuestion: Let $X$ denote the random variable associated to the plaintexts and $Y$ the random variable associated to the corresponding ciphertexts. If a cryptosystem achieves perfect secrecy, then we have that \\\\dots?\\n\\nOptions:\\nA. $\\\\Pr [X=x|Y=y] = \\\\Pr[Y=y]$.\\nB. $\\\\Pr [X=x|Y=y] = \\\\Pr[X=x]$.\\nC. $\\\\Pr [X=x|Y=y] = \\\\Pr[X=x,Y=y]$.\\nD. $\\\\Pr [X=x] = \\\\Pr[Y=y]$.<|endoftext|',\n",
       " 'chosen': '<|assistant|>\\nThe correct statement is: \\\\\"$\\\\\\\\\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\\\\\\\\\Pr[X=x]$.\\\\\"\\\\n\\\\nPerfect secrecy in a cryptosystem means that observing the ciphertext does not give any information about the plaintext. Mathematically, this is formalized as $\\\\\\\\\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\\\\\\\\\Pr[X=x]$, meaning that the probability of a certain plaintext being the true message given the ciphertext is the same as the probability of that plaintext being the true message without knowing the ciphertext.\\\\n\\\\nThe other statements are not necessarily true in the context of perfect secrecy. \\\\\"$\\\\\\\\\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\\\\\\\\\Pr[Y=y]$\\\\\" is not correct because if the probability of the plaintext given the ciphertext was equal to the probability of the ciphertext, it would mean that the ciphertext reveals information about the plaintext. \\\\\"$\\\\\\\\\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\\\\\\\\\Pr[X=x,Y=y]$\\\\\" is also not correct because it implies that the probability of the plaintext given the ciphertext is the same as the joint probability of the plaintext and ciphertext, which may not be true. Finally, \\\\\"$\\\\\\\\\\\\\\\\Pr [X=x] = \\\\\\\\\\\\\\\\Pr[Y=y]$\\\\\" is not necessarily true as well because perfect secrecy does not require the marginal probabilities of the plaintexts and ciphertexts to be equal.<|endoftext|',\n",
       " 'rejected': '<|assistant|>\\nI apologize for the mistake in my previous response. The correct statement should be: \\n\\nCorrect statement: \"$\\\\\\\\Pr [X=x|Y=y] = \\\\\\\\Pr[X=x]$.\"\\n\\nJustification: In a cryptosystem that achieves perfect secrecy, the knowledge of the ciphertext does not reveal any information about the plaintext. Therefore, the probability of a specific plaintext given a specific ciphertext should be the same as the probability of that specific plaintext occurring. This is reflected in the correct statement. Thank you for pointing out the error.<|endoftext|'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check:\n",
    "dataset_path = dataset_path = \"datasets/dpo_preference_data_self_made/train_preference_data\"\n",
    "loaded_dataset = load_from_disk(dataset_path)\n",
    "print(f\"Number of examples in the training set: {len(loaded_dataset['train'])}\")\n",
    "print(f\"Number of examples in the evaluation set: {len(loaded_dataset['eval'])}\")\n",
    "print(f\"Number of examples in the test set: {len(loaded_dataset['test'])}\")\n",
    "loaded_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(converted_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
