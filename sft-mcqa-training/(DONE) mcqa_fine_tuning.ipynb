{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code origin\n",
    "#Author: Alexander Valentini\n",
    "\n",
    "from datasets import load_dataset,DatasetDict,Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM \n",
    "from peft import LoraConfig\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "#train_data_path = os.path.join(project_dir, 'data/SFT_data/sft_train_dataset_below_1024_chat_format_no_length.json')\n",
    "#vali_data_path = os.path.join(project_dir, 'data/SFT_data/sft_validation_dataset_below_1024_chat_format_no_length.json')\n",
    "previous_checkpoint_path = None\n",
    "\n",
    "train_data_path = 'datasets/mcqa/mcqa_train_dataset_chattemplate_mcqa.jsonl'\n",
    "vali_data_path = 'datasets/mcqa/mcqa_validation_dataset_chattemplate_mcqa_halved.jsonl'\n",
    "#check if path is correct using os.path.exists()\n",
    "#print(os.path.exists(train_data_path))\n",
    "#print(os.path.exists(vali_data_path))\n",
    "\n",
    "dataset=load_dataset('json', data_files={\"train\":train_data_path, \"validation\":vali_data_path})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8708db9d49f64842b89e909878531080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name='AlexVal/dpo_model'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    attn_implementation=\"sdpa\"\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|pad|>\n",
      "50277\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"<|assistant|>\\n\" \n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.05,\n",
    "        r=128,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"mcqa_model\", # directory to save and repository id\n",
    "    num_train_epochs=1,                     \n",
    "    per_device_train_batch_size=8,          \n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            #Alexander: use gradient checkpointing to save memory - from tutorial\n",
    "    optim=\"adamw_torch_fused\",              # Alexander: use fused adamw optimizer for faster training\n",
    "    logging_steps=10,                       # Alexander: log every 10 steps for better debugging\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps= 1000,                  \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps= 500,            \n",
    "    save_total_limit=3,                     \n",
    "    load_best_model_at_end=True,            #Load best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",      # metric to use for best model\n",
    "    learning_rate=5e-7,                     # Lower learning rate to avoid \n",
    "#    fp16=True,                             \n",
    "    #bf16=True,                              \n",
    "    #tf32=True,                              \n",
    "    warmup_ratio=0.1,                      \n",
    "    lr_scheduler_type=\"linear\",           # use constant learning rate scheduler\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b67528943c4430b8ad8b1ea902afdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1458 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb20706ece7458eabb791b53eff2170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/182 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8296f9136044279f8087cea9c4e242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "d:\\ProgramData\\Anaconda3\\envs\\MNLP\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    "    data_collator=collator,\n",
    "#    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# start training, the model will be automatically saved to the output directory\n",
    "if previous_checkpoint_path is not None:\n",
    "    trainer.train(resume_from_checkpoint=previous_checkpoint_path)\n",
    "else:\n",
    "    trainer.train()\n",
    "\n",
    "trainer.save_model()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
