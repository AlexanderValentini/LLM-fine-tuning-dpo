{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data \n",
    "Change \"mcqa_dataset.jsonl\" to chat templet formate for awq quantization calibration\n",
    "\n",
    "ex. \n",
    "```\n",
    "[\n",
    "    \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me who you are.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I am a large language model named Qwen...\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code origin\n",
    "#Author: Alexander Valentini\n",
    "\n",
    "#This is to preprocess the data for the quantization. It sets up the data in the format with \"message\" and different \n",
    "#roles but does not add chattemplate.\n",
    "\n",
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install -U huggingface_hub\n",
    "# !pip install filelock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,DatasetDict,Dataset\n",
    "\n",
    "# dataset_path=\"/datasets/mcqa_dataset.jsonl\"\n",
    "train_dataset_path=\"datasets/mcqa/no_chattemplate/all_subjects_no_chattemplate/mcqa_train_dataset_full.jsonl\"\n",
    "test_dataset_path=\"datasets/mcqa/no_chattemplate/all_subjects_no_chattemplate/mcqa_test_dataset_full.jsonl\"\n",
    "# read the dataset\n",
    "mcqa_dataset=load_dataset(\"json\", data_files={\"train\":train_dataset_path, \"test\":test_dataset_path})\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # print(examples)\n",
    "    question=examples['question']\n",
    "    answer=examples['answer']\n",
    "    system=examples['subject']\n",
    "    final={\"message\":[\n",
    "        {\"role\": \"system\", \"content\": f\"You are a helpful {system} assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]}\n",
    "    return final\n",
    "\n",
    "#Dataset is loaded in as a dictionary of datasets. We create a new dataset dict from this:\n",
    "new_dataset=DatasetDict()\n",
    "for set in mcqa_dataset:\n",
    "    data=mcqa_dataset[set]\n",
    "    new_dataset[set]=data.map(preprocess_function)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists('datasets/mcqa/no_chattemplate/all_subjects_no_chattemplate/mcqa_train_dataset_full.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mcqa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save_path=\"datasets/mcqa/message_format_no_chattemplate/all_subjects_full_dataset/mcqa_train_dataset_full_message_format_no_chattemplate.jsonl\"\n",
    "test_save_path=\"datasets/mcqa/message_format_no_chattemplate/all_subjects_full_dataset/mcqa_test_dataset_full_message_format_no_chattemplate.jsonl\"\n",
    "#save new dataset train list to jsonl file\n",
    "#new_dataset['train'].to_json(train_save_path,orient='records',lines=True)\n",
    "#new_dataset['test'].to_json(test_save_path,orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['subject', 'question', 'answer', 'message'],\n",
       "        num_rows: 12686\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['subject', 'question', 'answer', 'message'],\n",
       "        num_rows: 1586\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the dataset\n",
    "new_mcqa_dataset=load_dataset(\"json\", data_files={\"train\":train_save_path, \"test\":test_save_path})\n",
    "new_mcqa_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "d:\\ProgramData\\Anaconda3\\envs\\MNLP\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b27bf1ca3ab4dd484872cdf8d20474b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385ed05dd0ec415ab6ccd4fe9d6b745a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Specify paths and hyperparameters for quantization\n",
    "model_path = \"AlexVal/dpo_model\"\n",
    "current_time=time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "quant_path = f\"models/quantized_model_from_exploration_script/quantized_model_{current_time}\"\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMV\" } # or version: GEMV\n",
    "\n",
    "# Load your tokenizer and model with AutoAWQ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, device_map=\"auto\", safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,DatasetDict,Dataset\n",
    "train_save_path=\"datasets/mcqa/message_format_no_chattemplate/all_subjects_full_dataset/mcqa_train_dataset_full_message_format_no_chattemplate.jsonl\"\n",
    "test_save_path=\"datasets/mcqa/message_format_no_chattemplate/all_subjects_full_dataset/mcqa_test_dataset_full_message_format_no_chattemplate.jsonl\"\n",
    "mcqa_train_data=load_dataset(\"json\", data_files=train_save_path)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for content in mcqa_train_data:\n",
    "    msg = content['message']\n",
    "    text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)\n",
    "    data.append(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.quantize(tokenizer, quant_config=quant_config, calib_data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "current_time=time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "quant_path = f\"models/quantized_model_from_exploration_script/quantized_model_{current_time}\"\n",
    "model.save_quantized(quant_path, safetensors=True, shard_size=\"4GB\")\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantization model size (gb): 1.7079910039901733\n"
     ]
    }
   ],
   "source": [
    "#Check quantized model size\n",
    "import glob\n",
    "import os\n",
    "quantized = False\n",
    "quantized_model_path=\"models/quantized_model_from_exploration_script/quantized_model_2024-06-05-19-23-32\"\n",
    "quantized_checkpoints = glob.glob(f\"{quantized_model_path}/*.bin\")\n",
    "if len(quantized_checkpoints) == 0:\n",
    "    quantized_checkpoints = glob.glob(f\"{quantized_model_path}/*.safetensors\")\n",
    "if len(quantized_checkpoints) == 0:\n",
    "    quantized_checkpoints = glob.glob(f\"{quantized_model_path}/*.pt\")\n",
    "\n",
    "quantized_model_size_on_disk = 0\n",
    "for name in quantized_checkpoints:\n",
    "    quantized_model_size_on_disk += os.path.getsize(name)\n",
    "quantized_model_size_gb=quantized_model_size_on_disk/(1024)**3\n",
    "print(\"quantization model size (gb):\",quantized_model_size_gb)\n",
    "\n",
    "# orig_checkpoints = glob.glob(f\"{self.policy_model_path}/*.bin\")\n",
    "# orig_model_size_on_disk = 0\n",
    "# for name in orig_checkpoints:\n",
    "#     orig_model_size_on_disk += os.path.getsize(name)\n",
    "# print(\"original model size\",orig_model_size_on_disk)\n",
    "# if quantized_model_size_on_disk < orig_model_size_on_disk:\n",
    "#     quantized = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1886693a7a6442d7bd1352a5ebafaa4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125623a729964389bd8cc0a02caedf19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa5cc34fd6f4cf0b7af1d80091dbac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dd6e3575354fda8599da7a09a3834a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c7372155484c03aeb288b77d30b3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40320f4b4df0499a834ec3c25bf4ca7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/581 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# check generation\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "quantized_model_repoid=\"AlexVal/mcqa_model_full_only_mcqa-awq_newest\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    quantized_model_repoid, # the quantized model\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_repoid)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "prompt = \"Question: What value of y makes y + 2.9 = 11 true?\\n\\nOptions:\\nA. 8.1\\nB. 8.9\\nC. 9.1\\nD. 13.9\\n\\nAnswer:\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful elementary_mathematics assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"response:\",response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "safetensors_path = 'models/quantized_model_from_exploration_script/quantized_model_2024-06-05-19-23-32'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(safetensors_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(safetensors_path,use_safetensors = True).to(device)\n",
    "repoid='AlexVal/mcqa_model_full_only_mcqa-awq_newest'\n",
    "\n",
    "access_token=\"\"\n",
    "\n",
    "model.push_to_hub(\n",
    " repo_id=repoid,\n",
    " commit_message=\"Commiting model\",\n",
    " token=access_token\n",
    ")\n",
    "\n",
    "tokenizer.push_to_hub(\n",
    " repo_id=repoid,\n",
    " commit_message=\"Commiting tokenizer\",\n",
    " token=access_token\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
